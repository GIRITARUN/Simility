import pandas as pd
import os
import numpy as np
import copy

# data=pd.read_csv("2018-03-21T09-2018-03-21T15.csv",quotechar="~",sep = ',')
#
# data_modified=data.loc[data["sim_updated_customer"]>1521625634000]
# data_modified=data_modified.sort_values(by=['sim_updated_customer'],ascending=False)
# data_modified=data_modified.drop_duplicates(subset=["eid"],keep='first')
# data=data.loc[~data["order_is_confirmed_fraud"].isna()]
# data=data.loc[~data["order_is_confirmed_good"].isna()]
# data=data.loc[~data["order_status"].isna()]
# data=data.loc[~data["gateway_engine"].isna()]

data=pd.read_csv("data_clean_raw.csv")

data=data[data.columns.drop(list(data.filter(regex='_ts_').columns))]
data=data[data.columns.drop(list(data.filter(regex='lifetime_count').columns))]
data=data[data.columns.drop(list(data.filter(regex='lifetime_sum').columns))]

data=data[~data['buyer_site_name'].str.contains("360Members")]
data=data[~data['buyer_contact_email'].str.contains("@premiumvibe.com")]
data=data[~data['buyer_contact_email'].str.contains("@360experience.com")]
data=data[~data['buyer_contact_email'].str.contains("@fullfilment.com")]
data=data[~data['buyer_contact_email'].str.contains("priority@stubhub.com")]
data=data[~data['buyer_contact_email'].str.contains("fulfillment.sac.emea@ticketbis.com")]
data=data[~data['transaction_was_performed_by_an_agent']==True]

data_copy=copy.deepcopy(data)

import scipy.stats as sc
for i in range(data.shape[1]):
    p_data = data.iloc[:, i].value_counts() / len(data.iloc[:, i])
    entropy = sc.entropy(p_data)
    if entropy==0:
        print(data.columns[i],entropy)
        data_copy=data_copy.drop(data.columns[i],axis=1)



null_counts=data.isna().sum()
null_counts.to_csv("null_counts.csv")


def crosstab(data,column1,column2,percentages=0):
    if percentages==0:
        print(pd.crosstab(data[column1],data[column2]))
    if percentages==1:
        print(pd.crosstab(data[column1], data[column2]).apply(lambda r: r/r.sum(), axis=1))
    if percentages==2:
        print(pd.crosstab(data[column1].isna(), data[column2]).apply(lambda r: r/r.sum(), axis=1))
    if percentages == 3:
        print(pd.crosstab(data[column1].isna(), data[column2]))

def counts(data,column):
    print(data[column].value_counts())


data_numeric=data._get_numeric_data()
data_numeric=data_numeric.fillna(data_numeric.median())
data_numeric=data_numeric[data_numeric.columns.drop(list(data_numeric.filter(regex='sim').columns))]
data_numeric=data_numeric[data_numeric.columns.drop(list(data_numeric.filter(regex='date').columns))]
data_numeric=data_numeric[data_numeric.columns.drop(list(data_numeric.filter(regex='create').columns))]

data_numeric_copy=copy.deepcopy(data_numeric)
def correlation(dataset, threshold):
    list=[]
    col_corr = set() # Set of all the names of deleted columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if corr_matrix.iloc[i, j] >= threshold:
                colname = corr_matrix.columns[i] # getting the name of column
                col_corr.add(colname)
                if colname in dataset.columns:
                    list.append([colname,corr_matrix.columns[j],corr_matrix.iloc[i, j]])
                    print(colname,"is removed because of high correlation with column", corr_matrix.columns[j],"correlation =",corr_matrix.iloc[i, j])
                    print("\n")
                    del dataset[colname] # deleting the column from the dataset
    list=pd.DataFrame(list)
    return(dataset,list)

data_no_corr,column_corr=correlation(data_numeric_copy,0.7)

data_modifed=copy.deepcopy(data)
for i in list(column_corr[0]):
    data_modifed=data_modifed.drop(i,axis=1)



bins_event_days_left=[0,10,20,30,40,50,60,70,80,100,110,120,130,140,150]

import matplotlib.pyplot as plt
from matplotlib.ticker import FormatStrFormatter
def plot_histo(data,bins,title):
    rows=len(data)
    fig, ax = plt.subplots()

    counts, bins, patches = ax.hist(data, facecolor='yellow', edgecolor='gray',bins=bins)


    # Set the ticks to be at the edges of the bins.
    ax.set_xticks(bins)
    # Set the xaxis's tick labels to be formatted with 1 decimal place...
    ax.xaxis.set_major_formatter(FormatStrFormatter('%0.1f'))

    # Label the raw counts and the percentages below the x-axis...
    bin_centers = 0.5 * np.diff(bins) + bins[:-1]
    for count, x in zip(counts, bin_centers):
        # Label the raw counts
        ax.annotate(str(count), xy=(x, 0), xycoords=('data', 'axes fraction'),
            xytext=(0, -18), textcoords='offset points', va='top', ha='center')

        # Label the percentages
        percent = '%0.2f%%' % (100 * float(count) /rows)
        ax.annotate(percent, xy=(x, 0), xycoords=('data', 'axes fraction'),
            xytext=(0, -32), textcoords='offset points', va='top', ha='center')


    plt.subplots_adjust(bottom=0.15)
    plt.title(title)
    plt.show()



data['buyer_account_created_timestamp']=pd.to_datetime(data['buyer_account_created'], unit='ms')
data['transaction_date_timestamp']=pd.to_datetime(data['transaction_date'], unit='ms')
data["buyer_age_sim"]=data['transaction_date_timestamp']-data['buyer_account_created_timestamp']
data['buyer_age_sim_days'] = data['buyer_age_sim'].dt.days



data['event_onsale_date_timestamp']=pd.to_datetime(data['event_onsale_date'], unit='ms')

data["days_since_onsale_sim"]=data['transaction_date_timestamp']-data['event_onsale_date_timestamp']
data['days_since_onsale_sim'] = data['days_since_onsale_sim'].dt.days

data['event_date_timestamp']=pd.to_datetime(data['event_date'], unit='ms')
data["events_days_left_sim"]=data['event_date_timestamp']-data['transaction_date_timestamp']
data['events_days_left_sim'] = data['events_days_left_sim'].dt.days

import seaborn as sns
sns.distplot( data.loc[data['order_is_confirmed_fraud']==1]["events_days_left_sim"] , color="red", label="Fraud data")
sns.distplot( data.loc[data['order_is_confirmed_fraud']==0]["events_days_left_sim"] , color="blue", label="Non Fraud Data")
plt.legend()
plt.show()

data['buyer_billing_identifier_bool'] = data['buyer_billing_identifier'].isna()
data['buyer_billing_identifier_type_imputed']=data['buyer_billing_identifier_type']
data.loc[data['buyer_billing_identifier_type'].isna(),"buyer_billing_identifier_type_imputed"]="missing"
data['buyer_billing_identifier_type_bool'] = data['buyer_billing_identifier_type'].isna()
data["buyer_billing_identifier_nulls"]=data[["buyer_billing_identifier","buyer_billing_identifier_type"]].isna().sum(axis=1)



data['event_date_timestamp']=pd.to_datetime(data['event_date'], unit='ms')
data["events_days_left_sim"]=data['event_date_timestamp']-data['transaction_date_timestamp']
data['events_days_left_sim'] = data['events_days_left_sim'].dt.days

data['seller_account_created_timestamp']=pd.to_datetime(data['seller_account_created'], unit='ms')
data['transaction_date_timestamp']=pd.to_datetime(data['transaction_date'], unit='ms')
data["seller_account_age_days_sim"]=data['transaction_date_timestamp']-data['seller_account_created_timestamp']
data['seller_account_age_days_sim'] = data['seller_account_age_days_sim'].dt.days


data["buyer_billing_nulls"]=data[["buyer_billing_city","buyer_billing_country","buyer_billing_phone","buyer_billing_state"]].isna().sum(axis=1)


data["buyer_contact_nulls"]=data[["buyer_contact_country","buyer_contact_postal_code","buyer_contact_state"]].isna().sum(axis=1)


data["buyer_contact_phone_missing"]=data["buyer_contact_phone"].isna()


data["buyer_delivery_nulls"]=data[["buyer_delivery_state","buyer_delivery_country"]].isna().sum(axis=1)
data["buyer_amount_sold_usd_bool"]=data["buyer_amount_sold_usd"].isna()


data["listing_nulls"]=data[["listing_row","listing_seat","listing_section"]].isna().sum(axis=1)



# import seaborn as sns
# sns.distplot( data.loc[(data['order_is_confirmed_fraud']==1) & (data["buyer_amount_sold_usd"]<100000 & ~data["buyer_amount_sold_usd"].isna())]["buyer_amount_sold_usd"] , color="red", label="Fraud data")
# sns.distplot( data.loc[(data['order_is_confirmed_fraud']==0) & (data["buyer_amount_sold_usd"]<100000 & ~data["buyer_amount_sold_usd"].isna())]["buyer_amount_sold_usd"] , color="blue", label="non Fraud data")
# plt.legend()
# plt.show()

features_to_check_info_value=["buyer_contact_email_avg_amount_value_per_email_1day","buyer_contact_email_email_handle_num_unique_chars","buyer_contact_email_email_kbmiddlerow_char_ratio","buyer_contact_email_email_length","buyer_contact_email_handle_kbdistance","buyer_contact_email_handle_nonalpharatio","buyer_contact_email_handle_repeatratio","buyer_contact_email_handle_vowelratio","buyer_contact_email_num_distinct_address_per_email_1day","buyer_contact_email_num_distinct_seller_per_email_7day","buyer_contact_email_num_distinct_user_per_email_7day","buyer_contact_email_numdays_between_be_transaction_firstseen_lastseen_by_email","buyer_contact_email_numdays_since_be_transaction_lastseen_by_email","buyer_contact_phone_num_distinct_seller_per_phone_7day","buyer_contact_phone_numdays_between_be_transaction_firstseen_lastseen_by_phone","buyer_contact_phone_numdays_since_be_transaction_lastseen_by_phone","buyer_user_id_num_distinct_phone_per_user_1day","full_delivery_address_address_kbdistance","full_delivery_address_address_length","full_delivery_address_address_nonalpharatio","full_delivery_address_address_num_unique_chars","full_delivery_address_address_prop_words_uppercase_start","full_delivery_address_address_vowelratio","full_delivery_address_avg_amount_value_per_address_7day","full_delivery_address_num_distinct_phone_per_address_30day","full_delivery_address_num_distinct_seller_per_address_1day","full_delivery_address_num_distinct_seller_per_address_30day","full_delivery_address_numdays_between_be_transaction_firstseen_lastseen_by_address","full_delivery_address_numdays_since_be_transaction_lastseen_by_address","full_delivery_address_total_amount_value_per_address_1day","full_delivery_address_total_amount_value_per_address_lifetime","seller_user_id_num_distinct_address_per_seller_30day","seller_user_id_num_distinct_phone_per_seller_1day","seller_user_id_num_distinct_phone_per_seller_30day","seller_user_id_numdays_between_be_transaction_firstseen_lastseen_by_seller","seller_user_id_numdays_since_be_transaction_lastseen_by_seller"]

data_features1=data[features]
from sklearn.feature_selection import mutual_info_classif

for i in data_features1.columns:
    try:
        print(i, mutual_info_classif(np.array(data_features1[i].fillna(0)).reshape(-1, 1),
                                     np.array(data_features1["order_is_confirmed_fraud"].astype("int")), n_neighbors=10))
    except:
        print(i, "not computed")

features_v1=["buyer_billing_is_enterprise","buyer_contact_email_num_distinct_address_per_email_1day","buyer_contact_email_num_distinct_seller_per_email_7day","buyer_contact_email_num_distinct_user_per_email_7day","buyer_contact_phone_num_distinct_seller_per_phone_7day","buyer_rejected_purchases_amount_usd","buyer_rejected_purchases_num","buyer_successful_purchases_num","buyer_user_id_num_distinct_phone_per_user_1day","buyer_users_with_same_email","event_root_category_name_en","event_source","full_delivery_address_address_kbdistance","full_delivery_address_address_length","full_delivery_address_address_nonalpharatio","full_delivery_address_address_num_unique_chars","full_delivery_address_address_prop_words_uppercase_start","full_delivery_address_address_vowelratio","full_delivery_address_avg_amount_value_per_address_7day","full_delivery_address_num_distinct_phone_per_address_30day","full_delivery_address_num_distinct_seller_per_address_1day","full_delivery_address_num_distinct_seller_per_address_30day","full_delivery_address_numdays_between_be_transaction_firstseen_lastseen_by_address","full_delivery_address_numdays_since_be_transaction_lastseen_by_address","full_delivery_address_total_amount_value_per_address_1day","full_delivery_address_total_amount_value_per_address_lifetime","gateway_engine","listing_desired_price","listing_is_instant_download","listing_ticket_format","order_is_confirmed_fraud","payment_form_is_custom","payment_method_is_offline","payment_method_is_refundable","payment_method_is_secure","seller_amount_sold_usd","seller_customer_classification","seller_seller_classification","seller_user_id_num_distinct_address_per_seller_30day","seller_users_with_same_email","transaction_num_tickets","transaction_total_amount_usd","buyer_billing_nulls","buyer_age_sim_days","buyer_contact_nulls","buyer_delivery_nulls","days_since_onsale_sim","events_days_left_sim","seller_account_age_days_sim","buyer_amount_sold_usd_bool","buyer_billing_identifier_nulls"]

def get_risky_scores(data,column,label,new_column_name,fractions_in_bucket=[0.2,0.6,0.2]):
    crosstab_df=pd.crosstab(data[column],data[label]).apply(lambda r: r/r.sum(), axis=1)
    crosstab_df=crosstab_df.sort_values(by=[True],ascending=False)

    risky=crosstab_df[False][0:int(crosstab_df.shape[0]*fractions_in_bucket[0])].index.tolist()
    non_risky=crosstab_df[False][int(-crosstab_df.shape[0]*fractions_in_bucket[2]):].index.tolist()
    data[new_column_name]="neutral"
    data.loc[data[column].isin(risky),new_column_name] = "risky"
    data.loc[data[column].isin(non_risky), new_column_name] = "non_risky"


get_risky_scores(data,"buyer_site_currency","order_is_confirmed_fraud","buyer_site_currency_modified")
get_risky_scores(data,"buyer_site_country","order_is_confirmed_fraud","buyer_site_country_modified")
get_risky_scores(data,"buyer_site_domain","order_is_confirmed_fraud","buyer_site_domain_modified")
get_risky_scores(data,"buyer_site_locale","order_is_confirmed_fraud","buyer_site_locale_modified")
