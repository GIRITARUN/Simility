import pandas as pd
import os
import numpy as np
os.chdir("/Users/tarungiri/PycharmProjects/simi2/travelstart")

file_list= list(os.listdir('./csv_files'))
for i in range(len(file_list)):
    globals()['data%s' % i] = pd.read_csv('csv_files/'+file_list[i],quotechar='~')
#different chucks of data:
frames=[data0,data1,data2,data3]
common_cols = list(set.intersection(*(set(df.columns) for df in frames)))
pd.concat([df[common_cols] for df in frames], ignore_index=True)


data=data[data.columns.drop(list(data.filter(regex='tombstone').columns))]
data=data[data.columns.drop(list(data.filter(regex='bad').columns))]
data=data[data.columns.drop(list(data.filter(regex='sim_ll').columns))]
data=data[data.columns.drop(list(data.filter(regex='lifetime_sum').columns))]
data=data[data.columns.drop(list(data.filter(regex='lifetime_count').columns))]

# print(data.filter(regex='date').columns)
# print(data.filter(like='date').columns)
# date_cols=data.filter(like='date').columns.tolist()
# data_no_date=data[data.columns.drop(list(data.filter(regex='date')))]

#common_cols= set(data.columns).intersection(data_jan22feb5.columns)

#sort by sim_updated_customer
data=data.sort_values(by=['sim_created_at'],ascending=True)
data=data.drop_duplicates(subset=["eid"],keep='first')
data[data["is_fraud"].isna(),"is_fraud"]=0


pd.merge(df_a, df_b, on='subject_id', how='left', suffixes=('_left', '_right'))

data_copy["label"]=1
data_copy.loc[data_copy["sim_dc_right"].isna(),"sim_dc_right"]=data_copy["sim_dc_left"]
data_copy.loc[data_copy["sim_dc_right"].isin(["{'No Fraud'}","{}","[u'No Fraud']","{'Review'}","[u'Review']"]),"label"]=0
data_copy.loc[data_copy["sim_dc_right"].isna(),"label"]=0


#null counts
null_counts=data.isnull().sum()

#different methods to get a subset with only numeric dtype to feed it to correlation function
# numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
# newdf = df.select_dtypes(include=numerics)

# df.select_dtypes(include=[np.number])

# list(df.select_dtypes(include=[np.number]).columns.values)
# u = set.intersection(*setlist)
# u = set.intersection(set(k) for k in files)
# frames=[data_common_cols,data_jan22feb5_common_cols]
# data_janfeb5_common_cols=pd.concat(frames)




#remove duplicate eid rows counts: first sort with sim_updated_customer
import pandas as pd
import numpy as np
df._get_numeric_data()
def find_correlation(data, threshold):
    corr_mat = data.corr()
    corr_mat.loc[:, :] = np.tril(corr_mat, k=-1)
    already_in = set()
    result = []
    for col in corr_mat:
        perfect_corr = corr_mat[col][corr_mat[col] > threshold].index.tolist()
        if perfect_corr and col not in already_in:
            already_in.update(set(perfect_corr))
            perfect_corr.append(col)
            result.append(perfect_corr)
    select_nested = [f[1:] for f in result]
    select_flat = [i for j in select_nested for i in j]
    return select_flat

data_no_num_corr=data[data.columns.drop(list(to_remove80['0']))]
fifty_percent_nulls=data_no_num_corr.columns[data_no_num_corr.isnull().mean() > 0.5]
data_no_num_corr_nulls_removed=data_no_num_corr[data_no_num_corr.columns.drop(list(fifty_percent_nulls))]
import scipy.stats as sc
for i in range(data_no_num_corr_nulls_removed.shape[1]):
    p_data = data_no_num_corr_nulls_removed.iloc[:, i].value_counts() / len(data_no_num_corr_nulls_removed.iloc[:, i])
    entropy = sc.entropy(p_data)
    print(data_no_num_corr_nulls_removed.columns[i],entropy)

#device data missing column
#email_domain - lowercase
#billing country code not equal to device country code after missing value imputation


data_no_num_corr_nulls_removed[data_no_num_corr_nulls_removed["is_fraud"].isna(),"is_fraud"]=0







to_remove80=to_remove80.drop(to_remove80.columns[0],axis=1)
data_no_num_corr=data[data.columns.drop(list(to_remove80['0']))]
from sklearn.feature_selection import mutual_info_classif
for i in data_numeric.columns:
    try:
        print(i,mutual_info_classif(np.array(data_numeric[i]).reshape(-1,1),np.array(data_numeric["is_fraud"].astype("int")),n_neighbors=10))
    except:
        print(i,"not computed")


import pandas
from scipy.stats import chi2_contingency

def chisq_of_df_cols(df, c1, c2):
    groupsizes = df.groupby([c1, c2]).size()
    ctsum = groupsizes.unstack(c1)
    # fillna(0) is necessary to remove any NAs which will cause exceptions
    return(chi2_contingency(ctsum.fillna(0)))



risky_routes=["ACC SYD","CPT MFE","LBV DKR","SUB KIX","NSI ABV","FIH STL","ASU JNB","BJM NBO","DMM TNR","JNB MSQ","LHR MEL","YYZ LHR","LOS HKG","SHO MPM","FBM HAV","CPT BLZ","LBV JNB","CMN KUL","MED RUH","LOS BJL","GRJ LHR","DAR DEL","JNB YYZ","DMM MED","JNB BLZ","LOS KAD","LOS LGW","HRE CPT","LOS QUO","JNB FCO","LOS GRU","LOS ABV","CAI CMB","NBO BOM","DMM MNL","CGK SIN","HOF CAI","LOS QOW","LOS TXL","NBO UKA","JNB ACC","LOS ACC"]
risky_email_domain=["ichaftraining","keromail","y7mail","utravel","atl","post","executivemail","yopmail","dr","kfu","contractor","protonmail","nam-sa","chef","fastmail","walla","yandex","email","gmai","engineer","homemail","laposte","maishasteel","pentravel","gmal","accountant","teleworm","ngindik","myself","nationaltickets","mail","usa","my","yaho","gmial","dayrep","flightcentre","rediffmail","omokarotravels","alj","mailinator","qq","hotmai"]
risky_isp=["Internux","LocalTel Communications","X-Host SRL","DoD Network Information Center","Movistar Colombia","FORTHnet SA","Virtual1 Limited","ZAINUGAS","Telecel","Linode","Primus Telecommunications","PT Telkom Indonesia","DurableDNS","LogicWeb Inc","Numericable","Bandcon","Choopa"," LLC","Iomart Hosting Limited","Scancom Ltd.","Total Server Solutions L.L.C.","Philippine Long Distance Telephone","Cogent Communications","Maroc Telecom","Bharti Airtel","Jio","Vodafone Kabel Deutschland","etisalat Nigeria","Real Time Technologies Alliance - Africa"]
risky_odo_origin=["ASU","MKE","NSI","FRU","SJC","SUB","BJM","BOG","BEG","CKY","FBM","SHO","CCU","SIN","PER","ATH","BLZ","DSS","IAD","ARN","DFW","LAX","PEK","BUQ","ISB","CGK"]
risky_odo_destination=["ASU","SUB","NSI","CKY","BOG","FBM","SEA","ARN","PEK","GRU","LAX","ACC"]
risky_company_codes=["gomicroapp","tsgoweb","jumiago","jumiang","tstrweb","wegoomapi","onedayonly","tsnaweb"]
risky_timezone=["Australia/Melbourne","Europe/Paris","America/New_York","Europe/Athens","America/Asuncion","America/Bogota","Asia/Jakarta","America/Argentina/Buenos_Aires","Africa/Casablanca","America/Los_Angeles","Africa/Accra","America/Phoenix","Asia/Manila","America/Chicago","Africa/Harare","Australia/Perth","Asia/Kolkata"]
risky_device_country=["GR","PY","CO","ID","AR","MA","SL","US","ZW","IN","GH","PH","UG","FR"]
risky_device_language=["el","es-mx","es-419","fr-ch","ru","en-in","ru-ru","en-ng","en","fr-fr","fr"]


data_no_num_corr_nulls_removed['email_domain']=data_no_num_corr_nulls_removed['email_domain'].str.lower()
data_no_num_corr_nulls_removed["email_domain"]=data_no_num_corr_nulls_removed["email_domain"].str.split('.').str.get(0)

data_no_num_corr_nulls_removed["risky_traveller_route"]=0
data_no_num_corr_nulls_removed.loc[data_no_num_corr_nulls_removed["traveller_route"].isin(risky_routes),"risky_traveller_route"]=1

data_no_num_corr_nulls_removed["risky_email_domain"]=0
data_no_num_corr_nulls_removed.loc[data_no_num_corr_nulls_removed["email_domain"].isin(risky_email_domain),"risky_email_domain"]=1

data_no_num_corr_nulls_removed["risky_device_id_ip_isp"]=0
data_no_num_corr_nulls_removed.loc[data_no_num_corr_nulls_removed["device_id_ip_isp"].isin(risky_isp),"risky_device_id_ip_isp"]=1


data_no_num_corr_nulls_removed["risky_odo_1_origin"]=0
data_no_num_corr_nulls_removed.loc[data_no_num_corr_nulls_removed["odo_1_origin"].isin(risky_odo_origin),"risky_odo_1_origin"]=1

data_no_num_corr_nulls_removed["risky_odo_1_destination"]=0
data_no_num_corr_nulls_removed.loc[data_no_num_corr_nulls_removed["odo_1_destination"].isin(risky_odo_destination),"risky_odo_1_destination"]=1

data_no_num_corr_nulls_removed["risky_company_code"]=0
data_no_num_corr_nulls_removed.loc[data_no_num_corr_nulls_removed["company_code"].isin(risky_company_codes),"risky_company_code"]=1

data_no_num_corr_nulls_removed["risky_device_id_ip_ews_time_zone"]=0
data_no_num_corr_nulls_removed.loc[data_no_num_corr_nulls_removed["device_id_ip_ews_time_zone"].isin(risky_timezone),"risky_device_id_ip_ews_time_zone"]=1

data_no_num_corr_nulls_removed["risky_device_id_ip_country"]=0
data_no_num_corr_nulls_removed.loc[data_no_num_corr_nulls_removed["device_id_ip_country"].isin(risky_device_country),"risky_device_id_ip_country"]=1

data_no_num_corr_nulls_removed["risky_device_language"]=0
data_no_num_corr_nulls_removed.loc[data_no_num_corr_nulls_removed["device_language"].isin(risky_device_language),"risky_device_language"]=1


data_no_num_corr_nulls_removed['device_country_billing_country_mismatch']=0
data_no_num_corr_nulls_removed.loc[(data_no_num_corr_nulls_removed['billing_address_country_code'] != data_no_num_corr_nulls_removed['device_id_ip_country']) &(~data_no_num_corr_nulls_removed['billing_address_country_code'].isna()) &(~data_no_num_corr_nulls_removed['device_id_ip_country'].isna()),'device_country_billing_country_mismatch' ] = 1


import copy
data_no_num_corr_nulls_removed["card_1_holder_card_type"]=data["card_1_holder_card_type"]
data_no_num_corr_nulls_removed["card_1_payment_channel"]=data["card_1_payment_channel"]
data_no_num_corr_nulls_removed["trip_type"]=data["trip_type"]

modified_datav1=copy.deepcopy(data_no_num_corr_nulls_removed)


include_v1=pd.read_csv("include_v2.csv")
modified_datav1=modified_datav1[list(include_v1["Feature"])]

feature_imputations=pd.read_csv("imputations_v1.csv")
for i in range(feature_imputations.shape[0]):
    if (feature_imputations["Feature"][i] in modified_datav1.columns):
        print(feature_imputations["Feature"][i],feature_imputations["imputation"][i])
        modified_datav1.loc[modified_datav1[feature_imputations["Feature"][i]].isna(), feature_imputations["Feature"][i]] = feature_imputations["imputation"][i]
        if (feature_imputations["imputation"][i] == 0):
            modified_datav1[feature_imputations["Feature"][i]]=modified_datav1[feature_imputations["Feature"][i]].astype("float")



from sklearn.ensemble import RandomForestClassifier
X=data_no_num_corr_nulls_removed.drop(["label","is_fraud_left","is_fraud_right","sim_dc_left","sim_dc_right"],axis=1)
Y=data_no_num_corr_nulls_removed["is_fraud"]
rc= RandomForestClassifier(max_depth=10,n_estimators=50,max_features=20)
X=X.fillna(0)
X=X[X.columns.drop(list(X.filter(regex='bad').columns))]
X=X[X.columns.drop(list(X.filter(regex='ml').columns))]
rc.fit(np.array(X),np.array(Y))
rc.feature_importances_



prediction.loc[prediction["sim_dc_right"]==[u'Confirmed Fraud'],"sim_dc_right"]="{'Confirmed Fraud'}"


from sklearn.ensemble import RandomForestClassifier
X=data_no_num_corr_nulls_removed.drop("is_fraud",axis=1)
Y=data_no_num_corr_nulls_removed["is_fraud"]
rc= RandomForestClassifier(max_depth=10,n_estimators=50,max_features=20)
rc.fit(np.array(X),np.array(Y))
rc.feature_importances_


for i in data_numeric_no_corr.columns:
    print(i,data_copy[i].corr(data_copy["traveller_booking_amount"]))
